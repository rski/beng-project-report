\chapter{Implementation}
\label{cha:imp}
There was a number of considerations while designing and building this solution. These ranged from time limitations to minimising the amount of chances in the original Testu01 library and producing a less complex solution. 

This chapter explains these limitations and the process of making choices in the design of the final implementation in order to best balance the solution between these limitations.

\section{Design Choices}
\subsection{Threads versus Processes}
\label{sec:processes}
The usual approach in parallelisation is to spawn threads, each of which handles a discrete part of the processing. This approach is favoured in larger projects, because it allows handling the complexity of multithreading internally, without the use of process wrappers.

The biggest advantage of threads is that they share address space. Results can be passed to the thread handler via conventional means, such as arrays. Since each test already puts its results in a specific array cell, this approach would seem natural.

Usually, thread programming is bug-prone because of the need for locking. Data corruption, deadlocks and loss of performance due to waiting for locked resources are common\cite{ers-artofunix}. Fixing these would increase overall time required to complete the project.

A multi-threaded Testu01 would not suffer from these problems. The result data of each test is already placed completely independently in an array, so locking would not be required.

However, there is a number of problems with using threads, the greatest of which is that the complexity of the changes in Testu01 would be quite high. 

The creators of Testu01 have in some cases built in it functionality that eases the decoupling of the separate tests in threads. In some others, their code is too tightly integrated together that a multi-threaded approach would require major restructuring. 

The generators are not shareable between threads. There is only one generator that the tests use in sequence. If it were to be used by threads, each thread would not get sequential increments along the generator state, effectively getting random sequences instead of deterministic ones and the test results would be skewed.

Thus the biggest challenge would be to ensure that every thread uses a separate generator. An array of generators could be used, but this would require building a function that using a single generator instance creates an array of them. This would increase complexity and potentially introduce unforeseen consequences. Restructuring the test suites so that they use the generator array would be required. Finally, the way that the result printing aggregators are called and behave would have to change. All these involve restructuring that would both require large amount of time and might introduce unforeseen consequences that could cause erroneous results.

The greatest advantage of multiple processes is that the generator separation is done automatically. A new generator for the test is created by the slave process in the exact same way that the generators are currently created.

The test suites are already designed so that only a specific test can be run when they are called. Adding the functionality needed to select a single test is trivial. Thus using processes only requires small changes in Testu01. This makes porting any future Testu01 improvements to the parallel implementation comparatively easier.

Furthermore, the process wrapper code can be written in a language other than C. Since it is not a time crucial part of the code, it can be written a higher level language such as Python, improving maintainability and extensibility.

Of course, processes have disadvantages of their own. The biggest one is the fact that the generator is set up separately for each process. This is explored further in the limitations section.

Extracting the test results from the processes will also have to be handled by the process master. Ideally they would be aggregated programmatically. Testu01 already provides this functionality and threads would be able to use it. Processes will not, so it will have to be duplicated by the process master.

Overall processes seem the better choice. Implementing them requires less time and debugging will be easier, both of which are very important under the time limitations for the project.

This was the most critical part of the design process. It had to be simplistic enough to be doable within the time limits and provide a working solution. If it were discovered near the final stages that this approach would not yield results, there would not be enough time for a second attempt. There was only time for one solution and the path to it had to guarantee results. The more sophisticated solutions by Suciu et al\cite{parallelism-paper2} would have been interesting to explore, but possibly would have not provided results. That was not a risk that could be taken.

Processes, although primitive in comparison to frameworks that would allow thread based decoupling of the generators, are easy to use and learn and would almost definitely provide a performance boost, at least for more primitive generators.

\subsection{Aggregating the results}
The original test suite prints out the results of each test right after it is finished. This is not the case with the parallel version.

The choice for the results of each test to be captured immediately was made, but processed and finished only once the whole suite is finished. This means that there is no indication the results until the very end. If the test suite were to be stopped no results could be obtained from the run.

This was done to maintain the same order printing order as the original suites. Printing the results in order earlier could only implemented with a thread and would require locking mechanisms to be placed.

This would not guarantee that the tests are printed close to the time that each test is completed. In order to uphold the test order, the results for the latter tests would have to be printed after the first tests of the suite are completed. If the earlier tests were ones of the last to be run in the parallel version, the results would only be printed then, thus presenting no significant advantage over waiting until the whole suite is complete. The benefits do not justify writing very complex and bug prone code.

More importantly, having a thread would impact performance. Waiting for the locked resources and the overhead from switching between the threads adds overhead and would definitely affect the performance.

\subsection{Process master-slave model}
A process based implementation requires a process master and a process slave. The process slaves are written in C.

Talk a bit about how the test number is passed to the process

\subsection{Processmaster Implementation Language}
Python was chosen as the implementation language due to a number of reasons. Probably the most important one is Python's portability. It is readily available in the platforms that Testu01 works and can handle multiprocessing in an OS agnostic way.

It is also easy to use and provides very useful tools that make interpreting the process results much easier than it would be in C. It was chosen over other similar languages such as Lua and Perl due to previous experience with it.

\subsection{Test Order}
In order to minimise the total time of the suite, the best order is to start the slowest tests first. Obviously the actual run time of the tests cannot be known a priori, but an estimation can be acquired.

Testu01 prints the total CPU time taken for each test. Using a simple generator a list of tests from slowest to fastest can be acquired. This will not be a representation of the actual times the tests will take since it will vary from generator to generator, but it will be good enough to sort the tests by run time.

This order will hold strictly if the generators add equal overhead to each test, which from reading the source of Testu01, seems to be the case. Even if the order is not strictly correct, it still is a good estimate.

Running the tests by order restricts the flexibility of the solution. If new tests are added, simply changing the number of tests in the definitions will not be enough, the will have to be added to the order file as well. This is not a problem as tools to automatically generate the order from a file that holds the output of the test suites are provided.

\section{Building Testu01}
\subsection{Vanilla Testu01}
The project was entirely developed and tested on Arch Linux. As such, Arch specific tools were used to automate the build and install process, the most important of which is makepkg.

Testu01 can be built using the conventional ./configure make make install process. This is useful for Linux distributions for which packages with precompiled binaries do not exist. 

This is not the case for Arch Linux as a script that automates the process is provided in the Arch Linux User Repository. This way Testu01 is also handled by the package manager. The process for building packages from the AUR is explained thoroughly in the Arch Wiki\cite{archwiki-aur} and only the important parts for building Testu01 will be repeated here.

After acquiring the Testu01 PKGBUILD from the AUR\cite{testu01-aur}, it can be installed with the following commands
\begin{verbatim}
makepkg -s
sudo pacman -U testu01-1.2.3-3-$(uname -m).pkg.tar.xz
\end{verbatim}

\subsection{Modified Testu01}
Building the customised version of Testu01 only requires small changes in the PKGBUILD file. The source had to be changed to point to the correct github repository and the package name had to be changed as well.

The only required changes are the following:
\begin{verbatim}
source=("git://github.com/rski/testu01-parallel")
#since the source is no longer an archive there is no checksum
sha256sums=('SKIP')
#ensure that installing it along testu01 will not happen
conflicts=('testu01')
\end{verbatim}

The full PKGBUILD file is part of the source on the github repository\cite{github-repo} and can be found under the \texttt{/build} folder.

\section{Running the test suites}
After installing Testu01, the first step was to run the test suites using one of the built in generators.

The file \texttt{examples/bat1.c} packaged with Testu01 was compiled with \texttt{gcc bat.c -l testu01 -o bat1.out} and ran. All three suites were ran and timed using this executable. Part of their results can be found in the appendix and files with the full output on the repository\cite{githubrepo}.

This file was used as the basis for creating the test\_slave.out executable later.

\section{Modifying Testu01 for Parallelism}
The batteries in bbattery.c were already designed so that only one of the test functions could be called. Making the batteries run a single test function was merely a matter of writing a wrapper function that allows choosing one the three batteries and the test function to be run.

\begin{verbatim}
void startSingleTest(unif01_Gen * gen, int testNumber, int testSuite){
  //initiate the Rep array to 0
  int Rep[1+NDIM] = {0};
  //tests are run by looking at the Rep array
  //the ones that have a 0 value in their cell will not be run
  //here only the test testNumber is run
  Rep[testNumber] = 1;
  switch(testSuite){
    case 0:
      SmallCrush(gen, NULL, Rep);
      break;
    case 1:
      Crush(gen, Rep);
      break;
    case 2:
      BigCrush(gen, Rep);
      break;
    //only used for debugging purposes
    default:
      printf("Will run test %d from suite %d\n", testNumber, testSuite);
      break;
  }
} 
\end{verbatim}

Of course, the function declaration had to be added to the bbattery.tex file so that it would be placed in the generated header.

With this simple modification, selecting a single test from a specific suite is possible.

\section{Slave Process}
The previous function needs an executable to call it. This was created by using the bat1.c example file and adapting it appropriately.


\begin{verbatim}
int main(int argc, char **argv){
  //find out which test from which suite to run
  int testNumber, testSuite;
  //ensure that the program was called correctly
  checkArguments(argc, argv);
  testNumber = atoi(argv[1]);
  testSuite = atoi(argv[2]);
  //start up a generator and get a test running on it 
  translator's note keikaku means planunif01_Gen *gen;
  gen = ulcg_CreateLCG (2147483647, 16807, 0, 12345);
  //start the test & suite that was passed 
  startSingleTest(gen, testNumber, testSuite);
  ulcg_DeleteGen (gen);
  return 0;
}     
\end{verbatim}

This code is not particularly complex or innovative, but it is worth pointing it out to make clear how it is called and what it executes. It also serves as a guideline on how to write slave process executables that will be used to run the parallel suites.

A makefile is provided to compile the executable \texttt{test\_slave.out} from the source file \texttt{test\_slave.c}.

\section{Test Order Generation}

\section{Handling the Processes}
This was the most difficult and trickiest part of the project. It required forking processes with tests that had not run yet, capturing their results and ensuring that they finish properly before the results were collected. Corner cases that were not obvious in the beginning contributed to making this problem perilous. 

Initially, process forking was dealt with by the subprocess.call() function. It was usable, but had limitations and dangers that made a change to the Popen constructor imperative. The limitation was that even though the output of the subprocess could be captured, it could only be sent to a file handle. Writing to files would slow down the program and are not as an elegant solution as lists. This was not the reason for moving to Popen, as it could've been worked around. Using \texttt{stdout=PIPE} with this module could lead to a deadlock. It is impossible be sure that a deadlock would not happen, it could merely be said that one had not happened yet. For this reason, moving to Popen was important. Although this was discovered after the forking code had been implemented, the calls to the forking function had been placed in a wrapper function and the switch was relatively painless.

The hidden pitfall behind forking was a bug that would only appear in systems with a large number of cores. The list that holds the processes was allocated to be equal to the number of cores. This way the maximum number of tests could run in parallel. However, if the size of the suite was smaller than the number of cores, some of the cells of the process list would still be \texttt{None}. Thus, any subsequent attempts to call Popen member functions on them would cause the program to crash, since there were no safeguards around it. Creating a list whose size is the minimum of the number of cores and the test suite size fixed this issue:

\begin{verbatim}
def createProcessArray():
    if SUITE_SIZE[testSuite] <= numberOfCores:
        processes = [None] * SUITE_SIZE[testSuite]
    else:
        processes =  [None] * numberOfCores
    return processes
\end{verbatim}

Something worth mentioning is the generator that was used to get the next test to be run. These generators are not Random Number Generators, but Python objects that behave similarly to functions. Instead of returning a value like a function, generators yield one. Function state is destroyed once outside of the function scope, but generators resume where the last yield stopped. In a way they are similar to C style functions with static variables. A generator is used to get the next test that is to be executed. Once there are no more tests, it returns \texttt{None}.

\begin{figure}[h]
\begin{lstlisting}[language=Python]
def testGenerator(testSuite):
    #get the testOrder from the correspoinding file for the suite
    testOrder = testOrderFromFile(testSuite)
    for testNumber in testOrder:
        #list of the form ["command to be executed", testNumber, testSuiteNameString]
        test = ["./" + TEST_SLAVE_EXECUTABLE, testNumber, TEST_SUITES[testSuite]]
        yield test
    #if there are no more tests, the generator returns None
    while True:
        yield None
\end{lstlisting}
        \caption{The generator that yields the next test. From order.py}
        \label{fig:testOrderGenerator}
\end{figure}

The suite to be executed is held by a variable inside the main function of the file \texttt{process\_master.py}

This program follows a spawn-poll approach. The maximum number of processes are spawned. Then, every one second, every process in the process list is polled to determined if any have finished executing; if the have, their result is placed in the result array and new processes are spawned in their place:

\begin{figure}[h]
\begin{lstlisting}[language=Python]
#testGen is a python generator as explained above
def forkNewTests(processes, testGen, results):
  numOfProcesses = len(processes)
  nextTest = next(testGen)
  testNumber = int(nextTest[1])
  while(True):
    #if no tests left to run exit
    if not nextTest:
      break
    for i in range(0, numOfProcesses):
      #the or allows for allocating the new processes the first time this code is executed
      if (processes[i] is None or processes[i][0].poll() is not None) and nextTest:
        testNumber = int(nextTest[1])
        #get the results of the test to be replaced
        if processes[i] is not None:
          results[processes[i][1]-1],_ = processes[i][0].communicate()
        processes[i] = (forkTest(nextTest), testNumber)
        nextTest = next(testGen)
    sleep(1)
  return results, processes
\end{lstlisting}
\caption{The function that spawns the new processes from the file process\_master.py}
\label{fig:forkNewTests}
\end{figure}

This is more efficient that a fork-join implementation. Using joins would mean that each set of processes currently running would have to wait until the slowest of all the tests finishes. The time wasted is the difference in execution time between the slowest and fastest test. By polling every second, the maximum amount of time potentially wasted is one second. For BigCrush this is a difference that could impact execution time.

Once the forking function returns, a function that ensures that the last processes finish is called. After that the result aggregation function that is explained in the next section is called.

Having implemented these, the parallel Testu01 can now be run with the command:

\begin{verbatim}
python process_master.py
\end{verbatim}

The full source code for the python modules can be found in the github repository\cite{github-repo} under the folder \texttt{parallel}.

\section{Result Processing}

This part of the test suite is handled exclusively by the \texttt{results.py} module. It works quite well and achieves its purpose, but it build on code that would break if large changes are made in the Testu01 output.

The handleResults() function uses the list of all the individual test process results, the testSuite name and the total time to print an aggregate result output as close to the original Testu01 as possible.

One unexpected side effect of grabbing the output of the processes from stdout using pipes is that the results array does not hold strings, but a binary encoded version of the output. Once that was discovered it was a relatively easy fix and the rest of the result manipulation is done on strings.

To extract the data from the result list, the result of each one test is treated by splitting it into three entities:
\begin{itemize}
  \item Header: This is common for the whole suite, but each test process prints it. It is stripped from the result and generated again in the final output function.
  \item Test results: This is the information that needs to be extracted and printed for each test. 
  \item Summary: This is the last part of the result. Running the battery creates a summary, so again each process generates its own. It is stripped from the result, but data is also extracted from it.
\end{itemize}

The header is printed by \texttt{printSummary()}. It is just a function that imitates the original Testu01 summary function and has nothing clever in it.

It is more interesting to examine how the complete list of the failed tests is generated.

The function \texttt{stripHeaders()} converts the results from a single string that contains newline characters into a list. Each element of the list corresponds to a line and now the results can be parsed with regular expressions. This function also removes the first 12 lines as those are the lines that correspond to the header. The header is a fixed length so removing the first 12 lines rather than searching for the end of the header is adequate.

Extracting the tests that failed from the summary is particularly easy. It happens that the only lines that start optionally with whitespace followed by a number are the ones listing the failed tests.

\begin{figure}[h]
\begin{lstlisting}[language=Python]
def getFailedTests(summary):
  # a simple regex that looks for optional whitespace followed by a number
  ##then matches everything after the number
  testFailureRe = re.compile('\s*\d+..+')
  #each test process could have multiple tests failing 
  #this was not accounted for initially and the output of the parallel Testu01
  #did not match the serial one
  failedTests = []
  for line in summary:
    match = testFailureRe.match(line)
    if match is not None:
      failedTests.append(match.group())
  if failedTests:
    return failedTests
  else:
    return None       
\end{lstlisting}
\caption{The function that gathers the failed tests from the module results.py}
\label{fig:getFailedTests}
\end{figure}

In contrast to the header, the start of each test's summary varies. The function \texttt{stripSummaries()} calls \texttt{getStartOfSummary()} to determine from what element onwards the result should be stripped. \texttt{getStartOfSummary} uses regexes to find the line ``Generator state:'', as that always denotes the start of the summary.

Finally, using the information gathered from the results list, an output very similar to the one of the original Crush suites is printed.
