\chapter{Results and the Future}
\label{cha:results}
In this chapter, the results are analysed with regards to performance and correctness. Furthermore the limitations of this solution are discussed as well as possible improvements for the future.

\section{Assessing Correctness}
\subsection{Ensuring that the generators are separate}
Since the tests are run in separate processes, they do not use memory like threads. Each process has its own address space, independent of the others and so the generators are also separate. This can be easily verified by the printing the address of the generator after it is created:

\begin{figure}[h]
  \begin{verbatim}

  unif01_Gen *gen;
  gen = ulcg_CreateLCG (2147483647, 16807, 0, 12345);
  printf("generator address: %p\n", gen);

  \end{verbatim}
  \caption{code that creates a generator and prints the memory address from test\_slave.c}
  \label{fig:generator_address}
\end{figure}

Running the command \texttt{./test\_slave.out 1 1 \& ./test\_slave.out 1 1} starts two processes that both run the first test from the Crush suite. Their output is the following:

\begin{figure}[h]
  \begin{verbatim}
  generator address: 0x8454008
  generator address: 0x9af6008
  \end{verbatim}
  \caption{Values of pointers to two generators each created a concurrent process}
  \label{fig:generator_address_output}
\end{figure}

\subsection{Correspondence with serial results}
The result printing of parallel Testu01 is as close to the original suite as possible.

The output of each test is exactly the same. It is captured from the process's stdout and the header and the aggregate results are stripped from it. Then the results for each test are printed in the same order as in the serial batteries.

The aggregate results differ, but not greatly. There is a small difference in the whitespace and alignment of the failed test and eps columns, which might is not obvious to the eye, but could affect automatic parsers, if any such tools have been written for Testu01. It should be possible to account for those changes with only minor modifications. There are three fields which are missing from the Summary results; \texttt{Version}, \texttt{Generator} and \texttt{Number of statistics}. Finally, the total CPU time is calculated differently in the parallel version. There are small problems that can be fixed in a future release.

The list of failed tests between the two versions is comparable by inspection. For example, the Summary results for the smallCrush run serially and in parallel:

\begin{figure}[h]
\begin{verbatim}
========= Summary results of SmallCrush =========

 Version:          TestU01 1.2.3
 Generator:        ulcg_CreateLCG
 Number of statistics:  15
 Total CPU time:   00:00:07.20
 The following tests gave p-values outside [0.001, 0.9990]:
 (eps  means a value < 1.0e-300):
 (eps1 means a value < 1.0e-15):

       Test                          p-value
 ----------------------------------------------
  1  BirthdaySpacings                 eps
  2  Collision                        eps
  6  MaxOft                           eps
 ----------------------------------------------
 All other tests were passed
\end{verbatim}
\caption{serial Testu01 output for the ulcg\_CreateLCG generator}
  \label{fig:serial-small-output}
\end{figure}

\begin{figure}[h]
  \begin{verbatim}
========= Summary results of smallCrush =========
Total time: 4.030619s
The following tests gave p-values outside [0.001, 0.9990]:
(eps  means a value < 1.0e-300):
(eps1 means a value < 1.0e-15):
        Test                          p-value
 ----------------------------------------------
  1  BirthdaySpacings                 eps
  2  Collision                        eps
  6  MaxOft                           eps
 ----------------------------------------------
  \end{verbatim}
  \caption{parallel Testu01 output for the ulcg\_CreateLCG generator}
  \label{fig:parallel-small-output}
\end{figure}
\clearpage{}

By inspection, it is obvious that the same tests fail in both occasions. This can be seen in the Crush and BigCrush. Due to their length, they are available on the github repository\cite{github-repo}.

Although it can be verified by inspection that the same tests fail in both implementations, a tool that automates this process would be ideal. For this generator, more than 60 tests fail when running BigCrush. Automatically checking that they are tests that should fail would be faster.

Combining the fact that the failing tests are the same with having demonstrated that the generators are separate and produce valid sequences, it can be said with a large degree of confidence that the parallel Crushes are as correct as the serial ones.

\section{Testu01 Compatibility}
The code changes in this project are backwards compatible with Testu01. It was only required to add one function to the Testu01 source. No other modifications were needed and so the serial batteries can still be run in the exact same way as before.


\section{Limitations and Future Work}

\subsection{Generator setup time}
Currently, the biggest limitation of this solution is the fact that a test generator is initialised every time a test is run. Since the tests run in separate processes, a new generator is set up for each one.
For trivial generators, this cost might be insignificant, but for generators that are simulations of hardware, it can overshadow the performance gains of parallelisation. The following equations can help illustrate this point:

Linear test suite completion time: N * AverageTestTime + GenSetupTime
Parallel test suite completion time: N * (AverageTestTime + GenSetupTime) / NumbOfCores
Solving for GenSetupTime:

If this equation holds, then the performance of the parallel suite will be at worst the same as the serial one.
The number of cores of modern desktops and laptops ranges between two and eight. Using these numbers, estimates for the maximum efficient parallel GenSetupTime can be acquired. Since systems with more cores than eight were used in the actual measurements, estimations are included for those too.
Number of cores
2 4 6 8
smallCrush
Crush
BigCrush

The ideal solution would use copied generators as outlined in the Threads versus Processes section. At this point this is not possible and will require coming up with another solution that might be incompatible with the current one.

\subsection{Fast tests}
For suites comprised of small tests, running vanilla Testu01 might actually be faster.

New tests are allocated every 1 second. Thus tests processes that run for less time than that waste CPU time. Depending on the number of cores and what percentage of the 1 second is wasted, the sequential suites might be faster.

The minimum amount of times the allocation loop is called is equal to NumberOfTests mod NumberOfCores.

For processors with a large number of cores, more new processes will be spawned after every second so this effect is not as serious. If the whole suite can be run simultaneously it will take less than a second since new tests will be allocated only once. On the other hand, the run time on processors with a small number of cores, such as two, will be impacted more.

It would be an interesting extension to regulate the time between new test allocations depending on the number of cores detected. On the other hand, the tests whose runtime is smaller than one second are only part of the smallCrush suite, the total time of which is under 5 seconds on the machines tests. Thus writing code to optimise the parallelism of smaller tests might not be worth the effort.

\subsection{Lack of flexibility}
Currently a number of things are hardwired in the source code. This sped up the development process, but is not optimal from a usability point of view.

Providing a configuration file and/or command line arguments to allow custom files for the test order and the test\_slave executable would make the software more user friendly. At the moment, the test suite is changed by modifying the source code.

Providing a python file masked as a configuration file would solve these usability issues.


\subsection{Windows support}
The original test suites could be compiled for Windows. The Testu01 homepage provides precompiled Windows binaries which can be used to run the tests.

This project maintains compatibility with non-POSIX platforms, but the folder handling might not work as expected on Windows. Specifically, there is an explicit reference to the '/tmp' directory, which is used for storing the debug logs. This makes the code not portable to Windows.will not work on Windows.

With a few minor modifications, enough time and motivation it should still be possible to port this project on Windows.

\subsection{File generators}
Currently the three Crush suites allow for use of files as random number generators. This functionality has not been tested at all and was not taken under consideration when building the parallel Testu01. As a result, the best approximation that can be given for using the parallel suites with a file generator is simply undefined behaviour and should not be used at all.

However, given the nature of files, it would quite probably be possible to extend the parallelism for these generators as well without major restructuring of the code base.

\subsection{Repeating tests}
There are three wrapper functions of the form \texttt{bbattery\_RepeatCrush()}. These have not been re-implemented as it would be impossible. Restarting the suite with the process master provides a similar functionality.

\subsection{Conclusion}
This is not quite a perfect parallel implementation of Testu01. It is partial, but of a part that has not been parallelised before.

Despite its limitations, it introduces significant performance gains to the BigCrush battery and it produces the same results as the original Testu01.

In the future more sophisticated parallelisations of the Crush batteries with higher performance increases might be introduced. Until then this is a more than adequate replacement whose performance well justifies preference over the serial batteries.
